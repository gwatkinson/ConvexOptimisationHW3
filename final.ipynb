{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization - Homework 3\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "<i>Report plots, comments and theoretical results in a pdf file. Send your code together with the requested functions and a main script reproducing all your experiments. You can use Matlab, Python or Julia.</i>\n",
    "\n",
    "Given $x_1,...,x_n \\in \\mathbb{R}^d$ data vectors and $y_1,...,y_n \\in \\mathbb{R}$ observations, we are searching for regression parameters $w \\in \\mathbb{R}^d$ which fit data inputs to observations $y$ by minimizing their squared difference. In a high dimensional setting (when $n \\ll d$) a $\\ell_1$ norm penalty is\n",
    "often used on the regression coefficients $w$ in order to enforce sparsity of the solution (so that $w$ will only have a few non-zeros entries). Such penalization has well known statistical properties, and makes the model both more interpretable, and faster at test time. \n",
    "\n",
    "From an optimization point of view we want to solve the following problem called LASSO (which stands for Least Absolute Shrinkage Operator and Selection Operator)\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO}\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac{1}{2} \\left\\lVert Xw - y \\right\\rVert^2_2 + \\lambda \\left\\lVert w \\right\\rVert_1\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "in the variable $w \\in \\mathbb{R}^d$, where $X = (x^T_1, \\ldots, x^T_n) \\in \\mathbb{R}^{n\\times d},\\, y = (y_1, \\ldots, y_n) \\in \\mathbb{R}^n$ and $\\lambda > 0$ is a regularization parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navyblue'>\n",
    "1. Derive the dual problem of LASSO and format it as a general Quadratic Problem as follows\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{QP}\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & v^TQv + p^Tv \\\\\n",
    "\\text{subject to} & Av \\preceq b\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "in variable $v\\in \\mathbb{R}^n$, where $Q \\succeq 0$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repose the LASSO problem as the following problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO'}\n",
    "\\begin{array}{ll}\n",
    "\\displaystyle \\min_{w \\in \\mathbb{R}^d,\\, z \\in \\mathbb{R}^n} & \\frac{1}{2} \\left\\lVert z \\right\\rVert^2_2 + \\lambda \\left\\lVert w \\right\\rVert_1 \\\\\n",
    "\\text{subject to} & z = Xw - y \n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "The associated Lagragian is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(w, z, v) &= \\frac{1}{2} z^T z + \\lambda \\left\\lVert w \\right\\rVert_1 - v^T (y-Xw+z)\n",
    "\\end{align*}\n",
    "\n",
    "We can then calculate the dual function\n",
    "\n",
    "\\begin{align*}\n",
    "g(v) &= \\inf_{w, z} \\frac{1}{2} z^T z + \\lambda \\left\\lVert w \\right\\rVert_1 - v^T (y-Xw+z) \\\\\n",
    "&= - v^T y + \\inf_z \\{ \\frac{1}{2} z^T z - v^T z\\} + \\lambda \\inf_w \\{\\left\\lVert w \\right\\rVert_1 + \\frac{1}{\\lambda} v^T X w\\} \\\\\n",
    "&= \\left\\{ \\begin{array}{ll} - v^T y - \\frac{1}{2} v^T v & \\text{if } \\left\\lVert X^T v \\right\\lVert_{\\infty} \\le \\lambda \\\\\n",
    "- \\infty & \\text{otherwise} \\end{array}\\right.\n",
    "\\end{align*}\n",
    "\n",
    "We can obtain the dual problem \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO*}\n",
    "\\begin{array}{ll}\n",
    "\\text{maximize} & - v^T y - \\frac{1}{2} v^T v \\\\\n",
    "\\text{subject to} & \\left\\lVert X^T v \\right\\lVert_{\\infty} \\le \\lambda \\\\\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "which can be simplified as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO*}\n",
    "\\begin{array}{ll}\n",
    "\\displaystyle \\min_{v \\in \\mathbb{R}^n} & \\frac{1}{2} v^T v  + y^T v \\\\\n",
    "\\text{subject to} & - X^T v \\le \\lambda \\cdot \\mathbb{1}_d \\\\\n",
    "& X^T v \\le \\lambda \\cdot \\mathbb{1}_d\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we can rewrite it as \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{QP}\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & v^TQv + p^Tv \\\\\n",
    "\\text{subject to} & Av \\preceq b\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "with\n",
    "\n",
    "- $Q = \\frac{1}{2} I_n \\in \\mathbb{R}^{n \\times n}$, we have $Q \\succeq 0$.\n",
    "- $p = y \\in \\mathbb{R}^{n}$\n",
    "- $b = \\lambda \\cdot \\mathbb{1}_{2d} \\in \\mathbb{R}^{2d}$\n",
    "\n",
    "and \n",
    "\\begin{equation*}\n",
    "A = \\left(\n",
    "    \\begin{array}{c}\n",
    "    X^T \\\\\n",
    "    -X^T\n",
    "    \\end{array}\n",
    "\\right)\n",
    "\\in \\mathbb{R}^{2d \\times n}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "For the next question, we pose $m=2d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navyblue'>\n",
    "1. Impliment the barrier method to solve QP.\n",
    "\n",
    "- Write a function `v_seq = centering_step(Q, p, A, b, t, v0, eps)` which impliments the Newton method to solve the centering step given the inputs $(Q, p, A, b)$, the barrier method parameter $t$ (see lectures), initial variable $v_0$ and a target precision $\\epsilon$. The function outputs the sequence of variables iterates $(v_i)_{i=1, \\ldots, n_{\\epsilon}}$, where $n_{\\epsilon}$ is the number of iterations to obtain the $\\epsilon$ precision. Use a backtracking line search with appropriate parameters.\n",
    "- Write a function `v_seq = barr_method(Q, p, A, b, v0, eps)` which implements the barrier method to solve QP using precedent function given the data inputs $(Q, p, A, b)$, a feasible point $v_0$, a precision criterion $\\epsilon$. The function ouptuts the sequence of variables iterates $(v_i)_{i=1, \\ldots, n_{\\epsilon}}$, where $n_{\\epsilon}$ is the number of iterations to obtain the $\\epsilon$ precision.\n",
    "- Test your function on randomly generated matrices $X$ and observations $y$ withz $\\lambda = 10$. Plot precision criterion and gap $f(v_t) - f^*$ in semilog scale (using the best value found for $f$ as a surrogate for $f^*$). Repeat for different values of the barrier method parameter $\\mu = 2, 15, 50, 100, \\ldots$ and check the impact on $w$. What would be an appropriate choice for $\\mu$ ?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write\n",
    "\n",
    "$$\n",
    "A = \\left(\\begin{array}{ccc}\n",
    "& a_1^T & \\\\\n",
    "\\hline\n",
    "& \\vdots & \\\\\n",
    "\\hline\n",
    "& a_m^T &\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "with $a_i \\in \\mathbb{R}^n$.\n",
    "\n",
    "The barrier problem can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{Barrier}\n",
    "\n",
    "\\begin{array}{ll}\n",
    "\\displaystyle \\min_{v \\in \\mathbb{R}^n} & t (v^T Q v + p^T v) + \\phi(v) \\\\\n",
    "\\text{subject to} & \\forall i \\in [1, \\cdots, m], \\; a_i^T v - b_i \\le 0\n",
    "\\end{array}\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "We pose $\\forall i \\in [1, \\cdots, m], \\; f_i(v) =  a_i^T v - b_i \\in \\mathbb{R}$. With this notation, $\\phi(v) = - \\displaystyle \\sum_{i=1}^m \\log(-f_i(v))$.\n",
    "\n",
    "From there, we can look at\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\phi(v) &= \\sum_{i=1}^m \\frac{1}{- f_i(v)} \\nabla f_i(v) \\\\\n",
    "&= \\sum_{i=1}^m \\frac{1}{b_i - a_i^T v} a_i\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla^2 \\phi(v) &= \\sum_{i=1}^m \\frac{1}{f_i(v)^2} \\nabla f_i(v) \\nabla f_i(v)^T \\\\\n",
    "&= \\sum_{i=1}^m \\frac{1}{(b_i - a_i^T v)^2} a_i a_i^T\n",
    "\\end{align*}\n",
    "\n",
    "since $\\nabla^2 f_i(v) = 0$.\n",
    "\n",
    "\n",
    "Lastly, to get $w^*$ from the dual problem, we can use the fact that the Lagragian is minimized by the optimal point, the gradient of the Lagragian with respoect to $z$ vanishes. Which gives us:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla_z \\mathcal{L}(w^*, z^*, v^*) &= 0 \\\\\n",
    "    &= z^* - v^* \\\\\n",
    "    &= X w^* - y - v^*\n",
    "\\end{align*}\n",
    "\n",
    "Given $v^*, X$ and $y$, we can get $w^*$ by resolving the least squares :\n",
    "\n",
    "\\begin{align*}\n",
    "    w^* = X^{\\dagger}(y + v^*)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for the testing\n",
    "\n",
    "I decided to use Python to implement the testing on the data.\n",
    "\n",
    "The following code only a part of the total code I wrote to do this, I only kept the essential for readibility reasons.\n",
    "You can find the complete code on this GitHub repository: [https://github.com/gwatkinson/HW3](https://github.com/gwatkinson/HW3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapid implementation\n",
    "\n",
    "In this section, I implemented the mentioned function quite fast, and tested it on generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "f_type = Callable[[np.array], float]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the functions\n",
    "\n",
    "This cell defines the function of the problem, given the necessary matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(v, Q, p):\n",
    "    \"\"\"The original objective function f_0.\"\"\"\n",
    "    return v.T @ Q @ v + p.T @ v\n",
    "\n",
    "def grad_f(v, Q, p):\n",
    "    \"\"\"The gradient of f.\"\"\"\n",
    "    return 2 * Q @ v + p\n",
    "\n",
    "\n",
    "def hess_f(v, Q, p):\n",
    "    \"\"\"The hessian of f.\"\"\"\n",
    "    return 2 * Q\n",
    "\n",
    "\n",
    "def g(v, A, b, i):\n",
    "    \"\"\"The log affine constraints f_i.\"\"\"\n",
    "    ai = A[i, :]\n",
    "    bi = b[i]\n",
    "    return float(np.nan_to_num(-np.log(bi - ai.T @ v), nan=np.inf))\n",
    "\n",
    "def grad_g(v, A, b, i):\n",
    "    \"\"\"The gradient of the log affine constraints f_i.\"\"\"\n",
    "    ai = A[i, :]\n",
    "    bi = b[i]\n",
    "    return ai / (bi - ai.T @ v)\n",
    "\n",
    "def hess_g(v, A, b, i):\n",
    "    \"\"\"The hessian of the log affine constraints f_i.\"\"\"\n",
    "    ai = A[i, :]\n",
    "    bi = b[i]\n",
    "    return ai.reshape(-1, 1) @ ai.reshape(-1, 1).T / (bi - ai.T @ v) ** 2\n",
    "\n",
    "\n",
    "def phi(v, A, b):\n",
    "    \"\"\"The log barrier function phi.\"\"\"\n",
    "    m = len(b)\n",
    "    return np.sum([g(v, A, b, i) for i in range(m)])\n",
    "\n",
    "\n",
    "def grad_phi(v, A, b):\n",
    "    \"\"\"The gradient of the log barrier function.\"\"\"\n",
    "    m = len(b)\n",
    "    return np.sum(np.array([grad_g(v, A, b, i) for i in range(m)]), axis=0)\n",
    "\n",
    "\n",
    "def hess_phi(v, A, b):\n",
    "    \"\"\"The hessian of the log barrier function.\"\"\"\n",
    "    m = len(b)\n",
    "    return np.sum(np.array([hess_g(v, A, b, i) for i in range(m)]), axis=0)\n",
    "\n",
    "def dual_func(v, y):\n",
    "    \"\"\"The dual function (the inf of the Lagragian).\"\"\"\n",
    "    return - 0.5 * v.T @ v - v.T @ y\n",
    "\n",
    "def lasso_func(w, X, y, ld):\n",
    "    \"\"\"The lasso function.\"\"\"\n",
    "    return 0.5 * np.linalg.norm(X @ w - y)**2 + ld * np.linalg.norm(w, 1)\n",
    "\n",
    "def get_w_star(v_star, X, y):\n",
    "    return np.linalg.lstsq(X, y + v_star, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "This cell implements the algorithms used for resolving the convex problem.\n",
    "\n",
    "We first define the `backtracking_line_search` abd the `newton_method` that works on any function, given the gradient and hessian.\n",
    "\n",
    "Then, the `centering_step`, just applies the newton method to the constrained quadratic problem.\n",
    "\n",
    "Lastly, the `barr_method` implements the barrier method for a given $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_step(x: np.array, grad_f: f_type, hess_f: f_type) -> np.array:\n",
    "    step = -np.linalg.inv(hess_f(x)) @ grad_f(x)\n",
    "    return step\n",
    "\n",
    "def newton_decrement(x: np.array, grad_f: f_type, hess_f: f_type) -> float:\n",
    "    decrement = np.sqrt(grad_f(x).transpose() @ np.linalg.inv(hess_f(x)) @ grad_f(x))\n",
    "    return decrement\n",
    "\n",
    "\n",
    "def backtracking_line_search(\n",
    "    x: np.array, f: f_type, grad_f: f_type, delta_x: np.array,\n",
    "    alpha: float, beta: float, verbose: bool = False\n",
    ") -> float:\n",
    "    assert alpha > 0 and alpha < 1 / 2, \"Alpha must be between 0 and 0.5\"\n",
    "    assert beta > 0 and beta < 1, \"Beta must be between 0 and 1\"\n",
    "\n",
    "    t = 1\n",
    "    _ = 0\n",
    "    while f(x + t * delta_x) >= f(x) + alpha * t * grad_f(x) @ delta_x:\n",
    "        t *= beta\n",
    "        _ += 1\n",
    "        if _ >= 100:\n",
    "            raise ValueError(\"Backtracking not converging\")\n",
    "        \n",
    "    return t\n",
    "\n",
    "def newton_method(\n",
    "    x0: np.array,\n",
    "    f: f_type,\n",
    "    grad_f: f_type,\n",
    "    hess_f: f_type,\n",
    "    epsilon: float,\n",
    "    alpha: float = 0.1,\n",
    "    beta: float = 0.5,\n",
    "    verbose=True\n",
    "):\n",
    "\n",
    "    x = x0\n",
    "    decrement = newton_decrement(x, grad_f, hess_f)\n",
    "\n",
    "    xs = [x0]\n",
    "    decrements = [decrement]\n",
    "    steps = []\n",
    "    ts = []\n",
    "\n",
    "    _ = 0\n",
    "    while decrement**2 / 2 > epsilon:\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"\\tCriterion: {decrement**2:.4e}\")\n",
    "            decrement = newton_decrement(x, grad_f, hess_f)\n",
    "            step = newton_step(x, grad_f, hess_f)\n",
    "            t = backtracking_line_search(\n",
    "                x, f, grad_f, step, alpha, beta, verbose=verbose\n",
    "            )\n",
    "            x += t * step\n",
    "\n",
    "            decrements.append(decrement)\n",
    "            steps.append(step)\n",
    "            ts.append(t)\n",
    "            xs.append(x)\n",
    "\n",
    "            _ += 1\n",
    "            if _ >= 20:\n",
    "                raise ValueError(\"Newton method not converging\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            if verbose:\n",
    "                print(e)\n",
    "            break\n",
    "\n",
    "    return x, f(x), {\"xs\": xs, \"decrements\": decrements, \"steps\": steps, \"ts\": ts}\n",
    "\n",
    "def centering_step(Q, p, A, b, t, v0, eps, alpha=0.1, beta=0.5, verbose=True):\n",
    "    x_opt, f_opt, iterates = newton_method(\n",
    "        x0=v0,\n",
    "        f=lambda v: t * f(v, Q, p) + phi(v, A, b),\n",
    "        grad_f=lambda v: t * grad_f(v, Q, p) + grad_phi(v, A, b),\n",
    "        hess_f=lambda v: t * hess_f(v, Q, p) + hess_phi(v, A, b),\n",
    "        epsilon=eps,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    return x_opt, f_opt, iterates\n",
    "\n",
    "def barr_method(Q, p, A, b, v0, eps, t0=1, mu=15, alpha=0.1, beta=0.5, verbose=True):\n",
    "    t = t0\n",
    "    m = len(b)\n",
    "    v = v0\n",
    "    vs = [v]\n",
    "    ts = [t]\n",
    "    l_x_opt = []\n",
    "    l_f_opt = []\n",
    "    l_iterates = []\n",
    "\n",
    "    while m / t >= eps:\n",
    "        if verbose:\n",
    "            print(f\"t = {t} and m/t = {m/t: .2e}\")\n",
    "        x_opt, f_opt, iterates = centering_step(\n",
    "            Q, p, A, b, t, v, eps, alpha, beta, verbose=verbose\n",
    "        )\n",
    "        v = iterates[\"xs\"][-1]\n",
    "        t *= mu\n",
    "\n",
    "        vs.append(v)\n",
    "        ts.append(t)\n",
    "        l_iterates.append(iterates)\n",
    "\n",
    "    return vs, ts, l_iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data\n",
    "\n",
    "This cell generates random data for a regression model. It was taken from the [CVXPY documentation](https://www.cvxpy.org/examples/machine_learning/lasso_regression.html), and adapted for our needs.\n",
    "\n",
    "It defines a sparse $\\beta$ coefficient, then a random $X$ dataset.\n",
    "Then, it gives \n",
    "$$\n",
    "y = X \\beta + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "We can then define the other matrices of the problem $Q$, $p$, $A$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N=20, D=50, ld=10, sigma=5, density=0.2):\n",
    "    \"Generates data matrix X and observations Y.\"\n",
    "    np.random.seed(1)\n",
    "    beta_star = np.random.randn(D)\n",
    "    idxs = np.random.choice(range(D), int((1-density)*D), replace=False)\n",
    "    for idx in idxs:\n",
    "        beta_star[idx] = 0\n",
    "    X = np.random.randn(N,D)\n",
    "    y = X.dot(beta_star) + np.random.normal(0, sigma, size=N)\n",
    "    Q = 0.5*np.identity(N)\n",
    "    p = y\n",
    "    A = np.vstack((X.T, -X.T))\n",
    "    b = ld * np.ones(2*D)\n",
    "    return X, y, beta_star, Q, p, A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if the algorithms on the data\n",
    "\n",
    "This section tests the preceding function on generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "N = 20\n",
    "D = 50\n",
    "M = 2*D\n",
    "ld = 10\n",
    "\n",
    "X, y, beta_star, Q, p, A, b = generate_data(\n",
    "    N=N,\n",
    "    D=D,\n",
    "    ld=ld,\n",
    "    sigma=1,\n",
    "    density=0.2\n",
    ")\n",
    "\n",
    "v0 = np.zeros(N)\n",
    "t0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18892/1119104714.py:6: RuntimeWarning: invalid value encountered in sqrt\n",
      "  decrement = np.sqrt(grad_f(x).transpose() @ np.linalg.inv(hess_f(x)) @ grad_f(x))\n",
      "/tmp/ipykernel_18892/2105800071.py:19: RuntimeWarning: invalid value encountered in log\n",
      "  return float(np.nan_to_num(-np.log(bi - ai.T @ v), nan=np.inf))\n"
     ]
    }
   ],
   "source": [
    "# Optimise with the barrier method\n",
    "\"\"\"\n",
    "`vs` contains the variables iterates.\n",
    "`l_iterates` contains the other iterates generated during the centering steps.\n",
    "\"\"\"\n",
    "vs, ts, l_iterates = barr_method(\n",
    "    Q, p, A, b, v0, eps=1e-8, t0=t0, mu=20, alpha=0.4, beta=0.5, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73181408, -0.84530778, -0.5256388 , -2.35593477, -1.00330414,\n",
       "        0.71251307, -1.63990239, -0.39663128,  0.82584064, -1.04842947,\n",
       "       -1.0351969 ,  0.15396756, -0.92493193, -0.54019353, -0.10492115,\n",
       "        0.56742521, -0.44287174,  0.32671034, -4.19676048, -1.57367554])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The final optimal point\n",
    "v_star = vs[-1]\n",
    "v_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the found point is inside the constraints and that $ \\| Xv \\|_{\\infty} \\le \\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.698624126031063e-10"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v_star @ A.T - b).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-44.64089454531671"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Value of the quadratic function evaluated at the optimal point.\n",
    "f(v_star, Q, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmation with CVXPY\n",
    "\n",
    "In this section, we confirm the results by using a reliable Python library `CVXPY` that implements convex optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-44.640894550003495,\n",
       " array([ 0.73181407, -0.84530778, -0.52563881, -2.35593478, -1.00330414,\n",
       "         0.71251306, -1.63990241, -0.39663129,  0.82584064, -1.04842948,\n",
       "        -1.0351969 ,  0.15396756, -0.92493194, -0.54019352, -0.10492113,\n",
       "         0.5674252 , -0.44287173,  0.32671034, -4.19676049, -1.57367555]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "# Posing the dual problem\n",
    "x = cp.Variable(N)\n",
    "dual_prob = cp.Problem(\n",
    "    cp.Minimize(\n",
    "        cp.quad_form(x, Q) + p.T @ x\n",
    "        ),\n",
    "    [A @ x <= b]\n",
    ")\n",
    "\n",
    "# Solving the problem\n",
    "dual_prob.solve()\n",
    "\n",
    "# Printing the optimal solution\n",
    "dual_prob.value, x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the found optimal values are really close, less than $5 \\times 10^{-9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6867825176377664e-09"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference between custom implementation and CVXPy\n",
    "f(v_star, Q, p) - dual_prob.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the norm of the difference between the points is less than $5 \\times 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.244096323079521e-08"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference between custom implementation and CVXPy\n",
    "np.linalg.norm(v_star - x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVXPY problem on the LASSO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56.89234479583571,\n",
       " array([-1.22170091e-01,  2.33106305e-01,  3.05069133e-12,  9.95144039e-01,\n",
       "         4.30489640e-11, -1.49052417e-09,  7.70765039e-01, -3.69562834e-02,\n",
       "         4.50499297e-01,  6.39145197e-11, -1.76728545e-10, -2.69659959e-10,\n",
       "        -2.19494913e-10,  2.19467178e-10, -3.22221072e-10,  6.13117966e-01,\n",
       "        -7.98881058e-02,  2.67996097e-11, -1.27990539e-10,  1.62517184e-10,\n",
       "        -1.18188333e-10, -5.51384321e-02, -1.38765046e-10,  5.02293389e-09,\n",
       "         1.69297407e-11,  8.41264900e-11,  1.00744598e-09, -3.47707951e-10,\n",
       "        -1.25861671e-10,  2.78196906e-10, -1.29884686e-01,  4.31098148e-11,\n",
       "         2.31807852e-11,  2.80474347e-10,  2.87306996e-10, -1.35318009e-01,\n",
       "        -3.53974321e-10,  4.47161978e-11,  1.67566123e-10,  1.25711362e-08,\n",
       "        -9.53288255e-10,  3.32165358e-10,  2.46366077e-10,  3.86656620e-01,\n",
       "         8.09973212e-11, -7.08446043e-10,  1.82890079e-10,  6.72950570e-10,\n",
       "        -9.21521839e-11,  3.69660574e-10]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pose the LASSO problem\n",
    "w = cp.Variable(D)\n",
    "prob = cp.Problem(\n",
    "    cp.Minimize(\n",
    "        cp.norm2(X @ w - y)**2 + ld * cp.norm1(w)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Solve the problem\n",
    "prob.solve()\n",
    "\n",
    "# Optimal solution\n",
    "prob.value, w.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In this section, we will display plots resuming the optimisation process.\n",
    "\n",
    "The code implementing this is different from the functions above.\n",
    "I wanted to get more familiar with Object Oriented Programming with Python, so I made a package that implements the same method with classes.\n",
    "I checked that the ouptuts were the same for a given dataset.\n",
    "The plots come from there.\n",
    "\n",
    "You can view the entire code on this Github repository: [https://github.com/gwatkinson/HW3](https://github.com/gwatkinson/HW3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering step\n",
    "\n",
    "In this section, we will focus more on the centering step used on the objective function with $t=100$.\n",
    "\n",
    "![Centering step output](outputs/example_output_centering.png)\n",
    "\n",
    "\n",
    "<center>Example output of a centering step for mu=10, t=100, alpha=0.1 and beta=0.5</center>\n",
    "\n",
    "We clearly see the two parts of the method, as described in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other example\n",
    "\n",
    "On another note, I also used this method on a 2D function, given as an example in the course, to plot the descent.\n",
    "\n",
    "![alt text](outputs/comparaison_newton_gradient_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barrier method\n",
    "\n",
    "This section presents results of the barrier method.\n",
    "\n",
    "![Barrier method results](outputs/lasso_pb_outputs.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('oc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5024d70390308f4a14e385530250ac7e5b7daada1f8b6c9bf590dc6da0331d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
