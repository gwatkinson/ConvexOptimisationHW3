{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization - Homework 3\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "<i>Report plots, comments and theoretical results in a pdf file. Send your code together with the requested functions and a main script reproducing all your experiments. You can use Matlab, Python or Julia.</i>\n",
    "\n",
    "Given $x_1,...,x_n \\in \\mathbb{R}^d$ data vectors and $y_1,...,y_n \\in \\mathbb{R}$ observations, we are searching for regression parameters $w \\in \\mathbb{R}^d$ which fit data inputs to observations $y$ by minimizing their squared difference. In a high dimensional setting (when $n \\ll d$) a $\\ell_1$ norm penalty is\n",
    "often used on the regression coefficients $w$ in order to enforce sparsity of the solution (so that $w$ will only have a few non-zeros entries). Such penalization has well known statistical properties, and makes the model both more interpretable, and faster at test time. \n",
    "\n",
    "From an optimization point of view we want to solve the following problem called LASSO (which stands for Least Absolute Shrinkage Operator and Selection Operator)\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO}\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & \\frac{1}{2} \\left\\lVert Xw - y \\right\\rVert^2_2 + \\lambda \\left\\lVert w \\right\\rVert_1\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "in the variable $w \\in \\mathbb{R}^d$, where $X = (x^T_1, \\ldots, x^T_n) \\in \\mathbb{R}^{n\\times d},\\, y = (y_1, \\ldots, y_n) \\in \\mathbb{R}^n$ and $\\lambda > 0$ is a regularization parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navyblue'>\n",
    "1. Derive the dual problem of LASSO and format it as a general Quadratic Problem as follows\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{QP}\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & v^TQv + p^Tv \\\\\n",
    "\\text{subject to} & Av \\preceq b\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "in variable $v\\in \\mathbb{R}^n$, where $Q \\succeq 0$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repose the LASSO problem as the following problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO'}\n",
    "\\begin{array}{ll}\n",
    "\\displaystyle \\min_{w \\in \\mathbb{R}^d,\\, z \\in \\mathbb{R}^n} & \\frac{1}{2} \\left\\lVert z \\right\\rVert^2_2 + \\lambda \\left\\lVert w \\right\\rVert_1 \\\\\n",
    "\\text{subject to} & z = Xw - y \n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "The associated Lagragian is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(w, z, v) &= \\frac{1}{2} z^T z + \\lambda \\left\\lVert w \\right\\rVert_1 + v^T (y-Xw+z)\n",
    "\\end{align*}\n",
    "\n",
    "We can then calculate the dual function\n",
    "\n",
    "\\begin{align*}\n",
    "g(v) &= \\inf_{w, z} \\frac{1}{2} z^T z + \\lambda \\left\\lVert w \\right\\rVert_1 + v^T (y-Xw+z) \\\\\n",
    "&= v^T y + \\inf_z \\{ \\frac{1}{2} z^T z + v^T z\\} + \\lambda \\inf_w \\{\\left\\lVert w \\right\\rVert_1 - \\frac{1}{\\lambda} v^T X w\\} \\\\\n",
    "&= \\left\\{ \\begin{array}{ll} v^T y - \\frac{1}{2} v^T v & \\text{if } \\left\\lVert X^T v \\right\\lVert_{\\infty} \\le \\lambda \\\\\n",
    "- \\infty & \\text{otherwise} \\end{array}\\right.\n",
    "\\end{align*}\n",
    "\n",
    "We can obtain the dual problem \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO*}\n",
    "\\begin{array}{ll}\n",
    "\\text{maximize} & v^T y - \\frac{1}{2} v^T v \\\\\n",
    "\\text{subject to} & \\left\\lVert X^T v \\right\\lVert_{\\infty} \\le \\lambda \\\\\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "which can be simplified as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{LASSO*}\n",
    "\\begin{array}{ll}\n",
    "\\displaystyle \\min_{v \\in \\mathbb{R}^n} & \\frac{1}{2} v^T v  - y^T v \\\\\n",
    "\\text{subject to} & - X^T v \\le \\lambda \\cdot \\mathbb{1}_d \\\\\n",
    "& X^T v \\le \\lambda \\cdot \\mathbb{1}_d\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we can rewrite it as \n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{QP}\n",
    "\\begin{array}{ll}\n",
    "\\text{minimize} & v^TQv + p^Tv \\\\\n",
    "\\text{subject to} & Av \\preceq b\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "- $Q = \\frac{1}{2} I_n \\in \\mathbb{R}^{n \\times n}$, we have $Q \\succeq 0$.\n",
    "- $p = -y \\in \\mathbb{R}^{n}$\n",
    "- $\n",
    "A = \\left(\\begin{array}{c}\n",
    "X^T \\\\\n",
    "\\hline -X^T\n",
    "\\end{array}\\right)\n",
    "\\in \\mathbb{R}^{2d \\times n}$\n",
    "- $b = \\lambda \\cdot \\mathbb{1}_{2d} \\in \\mathbb{R}^{2d}$\n",
    "\n",
    "For the next question, we pose $m=2d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='navyblue'>\n",
    "1. Impliment the barrier method to solve QP.\n",
    "\n",
    "- Write a function `v_seq = centering_step(Q, p, A, b, t, v0, eps)` which impliments the Newton method to solve the centering step given the inputs $(Q, p, A, b)$, the barrier method parameter $t$ (see lectures), initial variable $v_0$ and a target precision $\\epsilon$. The function outputs the sequence of variables iterates $(v_i)_{i=1, \\ldots, n_{\\epsilon}}$, where $n_{\\epsilon}$ is the number of iterations to obtain the $\\epsilon$ precision. Use a backtracking line search with appropriate parameters.\n",
    "- Write a function `v_seq = barr_method(Q, p, A, b, v0, eps)` which implements the barrier method to solve QP using precedent function given the data inputs $(Q, p, A, b)$, a feasible point $v_0$, a precision criterion $\\epsilon$. The function ouptuts the sequence of variables iterates $(v_i)_{i=1, \\ldots, n_{\\epsilon}}$, where $n_{\\epsilon}$ is the number of iterations to obtain the $\\epsilon$ precision.\n",
    "- Test your function on randomly generated matrices $X$ and observations $y$ withz $\\lambda = 10$. Plot precision criterion and gap $f(v_t) - f^*$ in semilog scale (using the best value found for $f$ as a surrogate for $f^*$). Repeat for different values of the barrier method parameter $\\mu = 2, 15, 50, 100, \\ldots$ and check the impact on $w$. What would be an appropriate choice for $\\mu$ ?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation avec CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem data.\n",
    "d = 30\n",
    "n = 20\n",
    "ld = 10\n",
    "\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(n, d)\n",
    "y = np.random.randn(n)\n",
    "\n",
    "Q = 0.5 * np.eye(n)\n",
    "p = -y\n",
    "A = np.vstack((X.transpose(), -X.transpose()))\n",
    "b = ld * np.ones(2*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 30), (60, 20), (60,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, A.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.8097915781266"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(n)\n",
    "x.T @ Q @ x + p.T @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the problem.\n",
    "w = cp.Variable(d)\n",
    "objective = cp.Minimize(0.5 * cp.sum_squares(X @ w - y) + ld * cp.norm1(w))\n",
    "constraints = []\n",
    "prob = cp.Problem(objective, constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.49638832e-16  2.41755363e-16  1.13010118e-17  9.80215835e-18\n",
      " -3.20979664e-17  1.51703003e-17 -1.88212106e-16  9.29312805e-18\n",
      "  3.09124339e-17 -1.84703512e-16  1.83428588e-16 -8.22773022e-17\n",
      "  4.07999310e-16  1.67822357e-16  4.96226074e-17 -2.01872854e-16\n",
      " -9.13446915e-17 -2.91577062e-16 -5.69568658e-02 -1.97740536e-17\n",
      "  2.15999264e-16  1.30042830e-16  2.75535912e-16  2.52588184e-16\n",
      "  1.29454912e-16  2.22406743e-16  7.53568342e-17 -2.34962309e-16\n",
      "  1.83025671e-17  7.87841190e-18]\n"
     ]
    }
   ],
   "source": [
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for x is stored in `x.value`.\n",
    "print(w.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "f_type = Callable[[np.array], float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(\n",
    "    x: np.array, f: f_type, grad_f: f_type, delta_x: np.array, alpha: float, beta: float\n",
    ") -> float:\n",
    "    assert alpha > 0 and alpha < 1 / 2, \"Alpha must be between 0 and 0.5\"\n",
    "    assert beta > 0 and beta < 1, \"Beta must be between 0 and 1\"\n",
    "\n",
    "    t = 1\n",
    "    while not (f(x + t * delta_x) < f(x) + alpha * t * grad_f(x) @ delta_x):\n",
    "        t *= beta\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def newton_step(x: np.array, grad_f: f_type, hess_f: f_type) -> np.array:\n",
    "    step = -np.linalg.inv(hess_f(x)) @ grad_f(x)\n",
    "    return step\n",
    "\n",
    "\n",
    "def newton_decrement(x: np.array, grad_f: f_type, hess_f: f_type) -> float:\n",
    "    decrement = np.sqrt(grad_f(x).transpose() @ np.linalg.inv(hess_f(x)) @ grad_f(x))\n",
    "    return decrement\n",
    "\n",
    "\n",
    "def newton_method(\n",
    "    x0: np.array,\n",
    "    f: f_type,\n",
    "    grad_f: f_type,\n",
    "    hess_f: f_type,\n",
    "    epsilon: float,\n",
    "    alpha: float = 0.1,\n",
    "    beta: float = 0.5,\n",
    "    verbose=True\n",
    "):\n",
    "\n",
    "    x = x0\n",
    "    decrement = newton_decrement(x, grad_f, hess_f)\n",
    "\n",
    "    xs = [x0]\n",
    "    decrements = [decrement]\n",
    "    steps = []\n",
    "    ts = []\n",
    "\n",
    "    while decrement**2 / 2 > epsilon:\n",
    "        if verbose: print(f\"Stopping value: {decrement**2:.2f} < {epsilon}\")\n",
    "        decrement = newton_decrement(x, grad_f, hess_f)\n",
    "        step = newton_step(x, grad_f, hess_f)\n",
    "        t = backtracking_line_search(x, f, grad_f, step, alpha, beta)\n",
    "        x += t * step\n",
    "\n",
    "        decrements.append(decrement)\n",
    "        steps.append(step)\n",
    "        ts.append(t)\n",
    "        xs.append(x)\n",
    "\n",
    "    return x, f(x), {\"xs\": xs, \"decrements\": decrements, \"steps\": steps, \"ts\": ts}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write\n",
    "\n",
    "$$\n",
    "A = \\left(\\begin{array}{ccc}\n",
    "& a_1^T & \\\\\n",
    "\\hline\n",
    "& \\vdots & \\\\\n",
    "\\hline\n",
    "& a_m^T &\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "with $a_i \\in \\mathbb{R}^n$.\n",
    "\n",
    "The barrier problem can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{Barrier}\n",
    "\n",
    "\\begin{array}{ll}\n",
    "\\displaystyle \\min_{v \\in \\mathbb{R}^n} & t (v^T Q v + p^T v) + \\phi(v) \\\\\n",
    "\\text{subject to} & \\forall i \\in [1, \\cdots, m], \\; a_i^T v - b_i \\le 0\n",
    "\\end{array}\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "We pose $\\forall i \\in [1, \\cdots, m], \\; f_i(v) =  a_i^T v - b_i \\in \\mathbb{R}$. With this notation, $\\phi(v) = - \\displaystyle \\sum_{i=1}^m \\log(-f_i(v))$.\n",
    "\n",
    "From there, we can look at\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\phi(v) &= \\sum_{i=1}^m \\frac{1}{- f_i(v)} \\nabla f_i(v) \\\\\n",
    "&= \\sum_{i=1}^m \\frac{1}{b_i - a_i^T v} a_i\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla^2 \\phi(v) &= \\sum_{i=1}^m \\frac{1}{f_i(v)^2} \\nabla f_i(v) \\nabla f_i(v)^T \\\\\n",
    "&= \\sum_{i=1}^m \\frac{1}{(b_i - a_i^T v)^2} a_i a_i^T\n",
    "\\end{align*}\n",
    "\n",
    "since $\\nabla^2 f_i(v) = 0$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(v, Q, p):\n",
    "    \"\"\"The original objective function f_0.\"\"\"\n",
    "    return v.transpose() @ Q @ v + p.transpose() @ v\n",
    "\n",
    "\n",
    "def grad_f(v, Q, p):\n",
    "    \"\"\"The gradient of f.\"\"\"\n",
    "    return (Q + Q.transpose()) @ v + p\n",
    "\n",
    "\n",
    "def hess_f(v, Q, p):\n",
    "    \"\"\"The hessian of f.\"\"\"\n",
    "    return Q\n",
    "\n",
    "\n",
    "def g(v, A, b, i):\n",
    "    \"\"\"The inequality constraints f_i.\"\"\"\n",
    "    aT = A[i, :]\n",
    "    return aT @ v - b[i]\n",
    "\n",
    "\n",
    "def grad_g(v, A, b, i):\n",
    "    \"\"\"The gradient of the nequality constraints f_i.\"\"\"\n",
    "    aT = A[i, :]\n",
    "    return aT.transpose()\n",
    "\n",
    "\n",
    "def phi(v, A, b):\n",
    "    \"\"\"The log barrier function phi.\"\"\"\n",
    "    m = len(b)\n",
    "    return -np.sum([np.log(-g(v, A, b, i)) for i in range(m)])\n",
    "\n",
    "\n",
    "def grad_phi(v, A, b):\n",
    "    \"\"\"The gradient of phi.\"\"\"\n",
    "    m = len(b)\n",
    "    l = [1 / (-g(v, A, b, i)) * grad_g(v, A, b, i) for i in range(m)]\n",
    "    return np.sum(l)\n",
    "\n",
    "\n",
    "def hess_phi(v, A, b):\n",
    "    \"\"\"The hessian of phi.\"\"\"\n",
    "    m = len(b)\n",
    "    l = [\n",
    "        1 / (g(v, A, b, i) ** 2) * grad_g(v, A, b, i) @ grad_g(v, A, b, i).transpose()\n",
    "        for i in range(m)\n",
    "    ]\n",
    "    return np.sum(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centering_step(Q, p, A, b, t, v0, eps, alpha=0.1, beta=0.5, verbose=True):\n",
    "    x_opt, f_opt, iterates = newton_method(\n",
    "        x0=v0,\n",
    "        f=lambda v: t * f(v, Q, p) + phi(v, A, b),\n",
    "        grad_f=lambda v: t * grad_f(v, Q, p) + grad_phi(v, A, b),\n",
    "        hess_f=lambda v: t * hess_f(v, Q, p) + hess_phi(v, A, b),\n",
    "        epsilon=eps,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    return iterates[\"xs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barr_method(Q, p, A, b, v0, eps, t0=1, mu=15, alpha=0.1, beta=0.5, verbose=True):\n",
    "    t = t0\n",
    "    m = len(b)\n",
    "    v = v0\n",
    "    vs = [v]\n",
    "\n",
    "    while m / t >= eps:\n",
    "        if verbose: print(\"t =\", t)\n",
    "        v = centering_step(Q, p, A, b, t, v, eps, alpha, beta, verbose=verbose)[-1]\n",
    "        t *= mu\n",
    "\n",
    "        vs.append(v)\n",
    "\n",
    "    return vs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 10\n",
    "n = 20\n",
    "d = 50\n",
    "X = np.random.randn(n, d)\n",
    "y = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 0.5 * np.eye(n)\n",
    "p = -y\n",
    "A = np.vstack((X.transpose(), -X.transpose()))\n",
    "b = lamb * np.ones((2*d))\n",
    "v0 = np.zeros(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 10\n",
      "Stopping value: 11.15 < 0.5\n",
      "Stopping value: 11.15 < 0.5\n",
      "t = 150\n",
      "Stopping value: 171.46 < 0.5\n",
      "Stopping value: 171.46 < 0.5\n",
      "Stopping value: 74.84 < 0.5\n",
      "Stopping value: 32.03 < 0.5\n",
      "Stopping value: 13.72 < 0.5\n",
      "Stopping value: 5.96 < 0.5\n",
      "Stopping value: 2.64 < 0.5\n",
      "Stopping value: 1.20 < 0.5\n"
     ]
    }
   ],
   "source": [
    "steps = barr_method(Q, p, A, b, v0, eps=0.5, t0=10, mu=15, alpha=0.1, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fs \u001b[39m=\u001b[39m [f(v, Q, p) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m steps]\n",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fs \u001b[39m=\u001b[39m [f(v, Q, p) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m steps]\n",
      "Cell \u001b[0;32mIn [31], line 3\u001b[0m, in \u001b[0;36mf\u001b[0;34m(v, Q, p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(v, Q, p):\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\"\"The original objective function f_0.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mreturn\u001b[39;00m v\u001b[39m.\u001b[39;49mtranspose() \u001b[39m@\u001b[39;49m Q \u001b[39m@\u001b[39m v \u001b[39m+\u001b[39m p\u001b[39m.\u001b[39mtranspose() \u001b[39m@\u001b[39m v\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "fs = [f(v, Q, p) for v in steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.11663611,  0.22020262,  0.65808571,  0.35437959, -0.05070135,\n",
       "         0.43086586,  0.60822425,  0.51965467,  0.07386646, -0.05863175,\n",
       "         0.61821539,  0.05333455,  0.90812095,  0.86598096,  0.18265423,\n",
       "        -0.0536297 ,  0.04928338,  0.48473493,  0.60911364,  0.84605453]),\n",
       " array([ 0.11663611,  0.22020262,  0.65808571,  0.35437959, -0.05070135,\n",
       "         0.43086586,  0.60822425,  0.51965467,  0.07386646, -0.05863175,\n",
       "         0.61821539,  0.05333455,  0.90812095,  0.86598096,  0.18265423,\n",
       "        -0.0536297 ,  0.04928338,  0.48473493,  0.60911364,  0.84605453]),\n",
       " array([ 0.11663611,  0.22020262,  0.65808571,  0.35437959, -0.05070135,\n",
       "         0.43086586,  0.60822425,  0.51965467,  0.07386646, -0.05863175,\n",
       "         0.61821539,  0.05333455,  0.90812095,  0.86598096,  0.18265423,\n",
       "        -0.0536297 ,  0.04928338,  0.48473493,  0.60911364,  0.84605453])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('OptiConvexe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ab8bf0c146581cfe6a2defaa83c5cde48b716af579edf6635d37acac362a7a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
